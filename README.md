# ğŸ§  Dev Helper AI

A local AI-powered chatbot built with **NestJS** for backend and **React + Vite** for frontend, using the **Llama3 model via Ollama**. This project enables you to chat with an LLM without using any paid API services like OpenAI.

---

## âœ¨ Features

- Chat with a local LLM (Llama3) using Ollama
- Simple prompt/response interface
- Built fully with TypeScript
- No need for API keys or OpenAI account
- Fullstack app with React frontend and NestJS backend

---

## ğŸ”§ Tech Stack

| Layer     | Stack                         |
|-----------|-------------------------------|
| Frontend  | React + Vite + Tailwind CSS   |
| Backend   | NestJS + Axios                |
| Model     | Ollama + Llama3               |
| Language  | TypeScript                    |

---

## ğŸŸ¢ Startup Guide

### 1. Install Prerequisites

- **[Node.js](https://nodejs.org/)** (v18 or above)
- **[Ollama](https://ollama.com/download)** (local LLM model runner)

---

### 2. Start Ollama & Pull Llama3

```bash
# Run this ONCE to pull and launch the model:
ollama run llama3
```

This will start the LLM API server at `http://localhost:11434`.

Leave it running while using the app.

---

### 3. Start the Backend (NestJS)

```bash
cd backend
npm install
npm run start
```

This launches the API server at `http://localhost:3000`, which connects to Ollama and exposes a `/chat` endpoint.

test your end point

```bash
curl -X POST http://localhost:3000/chat -H "Content-Type: application/json" -d '{"prompt": "can you draw an apple image for meï¼Ÿ"}
```

---

### 4. Start the Frontend (React + Vite)

In a new terminal tab:

```bash
cd frontend
npm install
npm run dev
```

The frontend runs at: [http://localhost:5173](http://localhost:5173)

---

### âœ… Youâ€™re Ready!

Open your browser â†’ go to `http://localhost:5173` â†’ enter a prompt â†’ get an AI-generated reply powered by Llama3 (via Ollama).

---

## ğŸ–¼ Screenshots

> _(Add screenshots here if available)_
> Example:
> ![screenshot](./screenshots/demo.png)

---

## ğŸ”Œ API Reference

### POST `/chat`

**Request Body:**

```json
{
  "prompt": "What is JavaScript?"
}
```

**Response:**

```json
{
  "reply": "JavaScript is a versatile programming language..."
}
```

---

## ğŸ“ Project Structure

```
dev-helper-ai/
â”œâ”€â”€ backend/             # NestJS backend
â”‚   â””â”€â”€ src/chat/        # Chat module
â”œâ”€â”€ frontend/            # React + Vite frontend
â”‚   â””â”€â”€ src/components/  # Chat UI component
```

---

## ğŸ¤” Why Ollama?

- No API keys or subscriptions
- Run everything locally, ideal for learning & privacy
- Works offline after downloading model

---

## ğŸ“„ License

This project is open source and available under the [MIT License](LICENSE).

---

## ğŸ™Œ Acknowledgements

- [Ollama](https://ollama.com) for local model serving
- [Llama3](https://llama.meta.com) by Meta
- [NestJS](https://nestjs.com) and [React](https://react.dev)
